#!/usr/bin/env ruby
$LOAD_PATH.unshift(File.join(File.dirname(__FILE__), '..', 'lib'))
require 'xupa_emec'

opts = Trollop::options do
  version "xupa emec #{XupaEmec::Version::STRING} (c) 2010 Bernardo de Pádua"
  banner <<-EOS
Xupe as informações do e-mec a partir de uma lista de IESs exportada no site.

Uso:
       xupa_emec -i rela.xls -o ies.csv
as opções são:
  EOS
  opt :entrada, "Arquivo fonte com lista de faculdades exportadas pelo emec", :short => 'i', :default => 'in.xls'
  opt :saida, "Arquivo csv que será gerado", :short => 'o', :default => 'out.csv'
  opt :quebraemail, "Gera uma linha por email", :short => 'q' 
  opt :buscacursos, "Busca lista de cursos das IES (demora mais)", :short => 'c' 
  opt :estimaalunos, "Estima numero de alunos somando o total de vagas por curso (demora muito mais)", :short => 's' 
end

crawler = XupaEmec::Crawler.new(:search_courses => (opts[:buscacursos] || opts[:estimaalunos]), :vacancies_estimation => opts[:estimaalunos] )

headers = ['nome', 'sigla', 'nome_limpo', 'tipo', 'cidade', 'tel', 'site', 'email', 'mantenedora', 'representante_nome', 'representante_primeiro_nome', 'representante_cargo']
headers << 'num_cursos' << 'lista_cursos' if opts[:buscacursos] || opts[:estimaalunos]
headers << 'total_vagas' << 'media_de_vagas_por_curso' if opts[:estimaalunos]

File.open(opts[:entrada], "r") do |input|

  FasterCSV.open(opts[:saida], "w", 
    :write_headers => true,
    :headers => headers) do |out_csv|

    in_html = doc = Nokogiri::HTML(input)
    iess_to_search = in_html.css('table:nth-child(2) tbody tr')

    puts "Vamos importar #{iess_to_search.size} IESs..."
    puts

    iess_to_search.each_with_index do |line, index|

      raw_name = line.css('td:nth-child(2)').text.strip
      ies_search_name = raw_name.split('-').max{|a,b| a.length <=> b.length }.strip #pega o nome maior
      
      puts
      puts "#{index+1} - Buscando nome da instituição '#{ies_search_name}'..."


      if opts[:quebraemail]
        ies_hash = crawler.crawl(ies_search_name)
        ies_hash['email'].split(',').each do |email|
          new_hash = ies_hash.clone
          new_hash['email'] = email
          out_csv << new_hash
        end
      else
        result = crawler.crawl(ies_search_name)
        out_csv << result if result
      end
      
    end

  end

end